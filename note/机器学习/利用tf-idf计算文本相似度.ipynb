{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0305294",
   "metadata": {},
   "source": [
    "利用tf-idf计算文本相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644dba81",
   "metadata": {},
   "source": [
    "词频TF，词频是一个词语在文章或句子中出现的次数。如果一个词很重要，很明显是应该在一个文章中出现很多次的，但是这也不是绝对的，比如“地”，“的”，“啊”等词，它们出现的次数对一篇文章的中心思想没有一点帮助，只是中文语法结构的一部分而已。这类词也被称为“停用词”。所以，在计算一篇文章的词频时，停用词是应该过滤掉的。\n",
    "TF=某个词在文档中出现的次数/该文档出现最多的词的出现次数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff0308",
   "metadata": {},
   "source": [
    "如果某个词比较少见（在我们准备的文章库中的占比比较少），但是它在这篇文章中多次出现，那么它很可能反映了这篇文章的特性，正是我们所需要的关键词。在此，在词频TF的基础上又引出了反文档频率IDF的概念。一般来说，在一篇文章或一个句子来说，对于每个词都有不同的重要性，这也就是词的权重。在词频的基础上，赋予每一个词的权重，进一步体现该词的重要性。比如一篇报道中国农业养殖的新闻报道。最常见的词（“的”、“是”、“在”）给予最小的权重，较常见的词（“国内”、“中国”、“报道”）给予较小的权重，较少见的词（“养殖”、“维基”）。所以刻画能力强的词语，权重应该是最高的。\n",
    "LDF=log(语料库文章总数/包含改词的文档数 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8f3c0",
   "metadata": {},
   "source": [
    "将TF和IDF进行相乘，就得到了一个词的TF-IDF值，某个词对文章重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。（在实际中，还要考虑词的词性等多维度的特性，动词，名词，形容词的刻画能力也是有所差别的；因社会热点而词的刻画性爆发式提高(比如 打call)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "729b180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['我', '不', '喜欢', '上海'], ['上海', '是', '一个', '好', '地方'], ['北京', '是', '一个', '好', '地方'], ['上海', '好吃', '的', '在', '哪里'], ['上海', '好玩', '的', '在', '哪里'], ['我', '喜欢', '上海', '的', '小吃'], ['上海', '路', '和', '上海', '人'], ['喜欢', '小吃']]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from gensim import corpora,models,similarities\n",
    "from collections import defaultdict\n",
    "doc0 = \"我不喜欢上海\"\n",
    "doc1 = \"上海是一个好地方\"\n",
    "doc2 = \"北京是一个好地方\"\n",
    "doc3 = \"上海好吃的在哪里\"\n",
    "doc4 = \"上海好玩的在哪里\"\n",
    "doc5 = \"我喜欢上海的小吃\"\n",
    "doc6 = \"上海路和上海人\"\n",
    "doc7 = \"喜欢小吃\"\n",
    "doc_test=\"我喜欢上海的小吃\"\n",
    "\n",
    "all_doc = []\n",
    "all_doc.append(doc0)\n",
    "all_doc.append(doc1)\n",
    "all_doc.append(doc2)\n",
    "all_doc.append(doc3)\n",
    "all_doc.append(doc4)\n",
    "all_doc.append(doc5)\n",
    "all_doc.append(doc6)\n",
    "all_doc.append(doc7)\n",
    "\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)\n",
    "print(all_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d9818048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用dictionary方法获取词袋（bag-of-words)\n",
    "dictionary = corpora.Dictionary(all_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11409017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 2, 1: 1, 2: 3, 0: 6, 7: 2, 4: 2, 6: 2, 5: 2, 8: 1, 11: 1, 12: 3, 10: 2, 9: 2, 13: 1, 14: 2, 17: 1, 16: 1, 15: 1}\n",
      "8\n",
      "36\n",
      "(0, '上海')\n",
      "(1, '不')\n",
      "(2, '喜欢')\n",
      "(3, '我')\n",
      "(4, '一个')\n",
      "(5, '地方')\n",
      "(6, '好')\n",
      "(7, '是')\n",
      "(8, '北京')\n",
      "(9, '哪里')\n",
      "(10, '在')\n",
      "(11, '好吃')\n",
      "(12, '的')\n",
      "(13, '好玩')\n",
      "(14, '小吃')\n",
      "(15, '人')\n",
      "(16, '和')\n",
      "(17, '路')\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# dictionary的一些用法：\n",
    "print(dictionary.dfs) #字典，{单词id，在多少文档中出现}\n",
    "print(dictionary.num_docs)#文档数目\n",
    "print(dictionary.num_pos)#所有词的个数\n",
    "for key in dictionary.items():\n",
    "    print(key)#(17183, '龙骨随葬') 单词id:单词\n",
    "print(dictionary.num_nnz) #每个文件中不重复词个数的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "34e2eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(0, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(0, 1), (9, 1), (10, 1), (12, 1), (13, 1)], [(0, 1), (2, 1), (3, 1), (12, 1), (14, 1)], [(0, 2), (15, 1), (16, 1), (17, 1)], [(2, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#对稀疏向量进行进一步处理，得到新的语料库\n",
    "corpus=[dictionary.doc2bow(doc) for doc  in all_doc_list]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5beddb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test_list = [word for word in jieba.cut(doc_test)]\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "434e58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将新的语料库通过tfidfmodel进行处理，得到tfidf\n",
    "#完成对corpus中出现的每一个特征的IDF值的统计工作即词语普遍重要性的度量,返回一个权重\n",
    "tfidf=models.TfidfModel(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3d3046ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到特征数\n",
    "#dictionary.token2id:字典，{词，对应的单词id}  dictionary.token2id.keys():单词个数\n",
    "featureNum=len(dictionary.token2id.keys())\n",
    "# #计算稀疏矩阵的相似性,稀疏矩阵相似度，从而建立索引，通过tfidf[corpus]和特征对应起来，\n",
    "# 则可直接找到相应的权重（相似度），也就是建立了索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b98d0aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.11893745470544932), (2, 0.4055078366880686), (3, 0.5731407639652386), (12, 0.4055078366880686), (14, 0.5731407639652386)]\n",
      "[0.4542352  0.01227498 0.         0.14231318 0.14231318 0.99999994\n",
      " 0.01876213 0.7020875 ]\n"
     ]
    }
   ],
   "source": [
    "index=similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featureNum)\n",
    "# #得到最终相似度结果\n",
    "print(tfidf[doc_test_vec])\n",
    "sim=index[tfidf[doc_test_vec]]\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "51593825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.99999994),\n",
       " (7, 0.7020875),\n",
       " (0, 0.4542352),\n",
       " (3, 0.14231318),\n",
       " (4, 0.14231318),\n",
       " (6, 0.01876213),\n",
       " (1, 0.012274977),\n",
       " (2, 0.0)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(enumerate(sim), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a780cf6",
   "metadata": {},
   "source": [
    "参考： https://www.cxybb.com/article/weixin_43758551/113918690"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
